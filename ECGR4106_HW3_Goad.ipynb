{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: Trevor Goad\n",
    "#Class: ECGR 4106\n",
    "#Assignment: HW 3\n",
    "#Date: 3/30/2022\n",
    "#github link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b469bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f9466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1589c43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "data_path = './tmp/tensorflow/mnist/input_data'\n",
    "cifar10 = datasets.CIFAR10(data_path, train = True, download = True,\n",
    "                          transform=transforms.ToTensor())\n",
    "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True,\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed77c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c3ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 ==0:\n",
    "            print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(),\n",
    "                                                        epoch,\n",
    "                                                        loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "409143d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 16:45:18.775685 Epoch 1, Training Loss 2.2204538946566372\n",
      "2022-03-30 16:46:38.553847 Epoch 10, Training Loss 1.4235408211608067\n",
      "2022-03-30 16:48:07.463176 Epoch 20, Training Loss 1.1908590451378347\n",
      "2022-03-30 16:49:36.109035 Epoch 30, Training Loss 1.0653670177892651\n",
      "2022-03-30 16:51:04.259801 Epoch 40, Training Loss 0.9823310786805799\n",
      "2022-03-30 16:52:35.120001 Epoch 50, Training Loss 0.9267054317552416\n",
      "2022-03-30 16:54:05.220083 Epoch 60, Training Loss 0.8880417294362012\n",
      "2022-03-30 16:55:34.247565 Epoch 70, Training Loss 0.8552308700349934\n",
      "2022-03-30 16:57:03.561531 Epoch 80, Training Loss 0.8270201234866286\n",
      "2022-03-30 16:58:32.665784 Epoch 90, Training Loss 0.8036015193785548\n",
      "2022-03-30 17:00:01.756657 Epoch 100, Training Loss 0.783330793918856\n",
      "2022-03-30 17:01:31.507640 Epoch 110, Training Loss 0.7633491739287706\n",
      "2022-03-30 17:03:00.325759 Epoch 120, Training Loss 0.7450339214118851\n",
      "2022-03-30 17:04:29.466556 Epoch 130, Training Loss 0.7305923602007844\n",
      "2022-03-30 17:05:58.146684 Epoch 140, Training Loss 0.717341734472748\n",
      "2022-03-30 17:07:26.616676 Epoch 150, Training Loss 0.7037348216756836\n",
      "2022-03-30 17:08:55.341370 Epoch 160, Training Loss 0.6936599489520577\n",
      "2022-03-30 17:10:24.344628 Epoch 170, Training Loss 0.6826403813288949\n",
      "2022-03-30 17:11:53.904205 Epoch 180, Training Loss 0.671760939195028\n",
      "2022-03-30 17:13:23.227522 Epoch 190, Training Loss 0.6622974283593085\n",
      "2022-03-30 17:14:52.329675 Epoch 200, Training Loss 0.653845779769256\n",
      "2022-03-30 17:16:22.262899 Epoch 210, Training Loss 0.6442729901246098\n",
      "2022-03-30 17:17:51.755076 Epoch 220, Training Loss 0.6386297370695397\n",
      "2022-03-30 17:19:20.501165 Epoch 230, Training Loss 0.6308389576652166\n",
      "2022-03-30 17:20:49.339499 Epoch 240, Training Loss 0.6240495255078806\n",
      "2022-03-30 17:22:19.428169 Epoch 250, Training Loss 0.6172586620006415\n",
      "2022-03-30 17:23:50.891687 Epoch 260, Training Loss 0.6103025387086527\n",
      "2022-03-30 17:25:20.302662 Epoch 270, Training Loss 0.6046152691859419\n",
      "2022-03-30 17:26:48.862979 Epoch 280, Training Loss 0.5982008956734787\n",
      "2022-03-30 17:28:17.511230 Epoch 290, Training Loss 0.5930597733734818\n",
      "2022-03-30 17:29:46.183388 Epoch 300, Training Loss 0.5877971783318483\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                          shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0c5e741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.78\n",
      "Accuracy val: 0.62\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted==labels).sum())\n",
    "                \n",
    "        print(\"Accuracy {}: {:.2f}\".format(name, correct/total))\n",
    "        \n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b7d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3,n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1,n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3069c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 17:32:27.345780 Epoch 1, Training Loss 2.3013628827946264\n",
      "2022-03-30 17:33:53.420263 Epoch 10, Training Loss 1.476427768807277\n",
      "2022-03-30 17:35:30.875132 Epoch 20, Training Loss 1.188119527095419\n",
      "2022-03-30 17:37:06.497807 Epoch 30, Training Loss 1.0329284735805238\n",
      "2022-03-30 17:38:42.209826 Epoch 40, Training Loss 0.9242095260516457\n",
      "2022-03-30 17:40:20.082832 Epoch 50, Training Loss 0.8534279653178457\n",
      "2022-03-30 17:41:59.169767 Epoch 60, Training Loss 0.8025574236727127\n",
      "2022-03-30 17:43:38.601437 Epoch 70, Training Loss 0.7635415945668964\n",
      "2022-03-30 17:45:17.958841 Epoch 80, Training Loss 0.7308549424419013\n",
      "2022-03-30 17:46:57.307985 Epoch 90, Training Loss 0.7032619550862276\n",
      "2022-03-30 17:48:38.141595 Epoch 100, Training Loss 0.6802099860080367\n",
      "2022-03-30 17:50:16.908770 Epoch 110, Training Loss 0.6603256349292252\n",
      "2022-03-30 17:51:53.597371 Epoch 120, Training Loss 0.6427061351211479\n",
      "2022-03-30 17:53:30.300080 Epoch 130, Training Loss 0.6271937866040203\n",
      "2022-03-30 17:55:06.456880 Epoch 140, Training Loss 0.6125812911621445\n",
      "2022-03-30 17:56:43.165091 Epoch 150, Training Loss 0.5997919177688906\n",
      "2022-03-30 17:58:19.525449 Epoch 160, Training Loss 0.5878807196150655\n",
      "2022-03-30 17:59:55.355988 Epoch 170, Training Loss 0.5773815752943153\n",
      "2022-03-30 18:01:32.165217 Epoch 180, Training Loss 0.5674758502437026\n",
      "2022-03-30 18:03:08.272460 Epoch 190, Training Loss 0.5581993020480246\n",
      "2022-03-30 18:04:44.190134 Epoch 200, Training Loss 0.5498404418454146\n",
      "2022-03-30 18:06:20.479163 Epoch 210, Training Loss 0.5417960118264189\n",
      "2022-03-30 18:07:57.589268 Epoch 220, Training Loss 0.5344154478415198\n",
      "2022-03-30 18:09:33.657470 Epoch 230, Training Loss 0.5274802735623192\n",
      "2022-03-30 18:11:10.413590 Epoch 240, Training Loss 0.5210184473401446\n",
      "2022-03-30 18:12:48.311542 Epoch 250, Training Loss 0.5150963444348491\n",
      "2022-03-30 18:14:24.834356 Epoch 260, Training Loss 0.5091303755407748\n",
      "2022-03-30 18:16:02.381010 Epoch 270, Training Loss 0.503531686389995\n",
      "2022-03-30 18:17:39.544047 Epoch 280, Training Loss 0.49873142359811634\n",
      "2022-03-30 18:19:16.327846 Epoch 290, Training Loss 0.4935858371808096\n",
      "2022-03-30 18:20:53.369305 Epoch 300, Training Loss 0.48857131968145173\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fefa94fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.81\n",
      "Accuracy val: 0.68\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a60f174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                            padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dc43c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "                    *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "132c7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 18:21:18.431655 Epoch 1, Training Loss 1.9942206347080143\n",
      "2022-03-30 18:23:38.070602 Epoch 10, Training Loss 0.8440433672017149\n",
      "2022-03-30 18:26:15.286502 Epoch 20, Training Loss 0.5992770129076356\n",
      "2022-03-30 18:28:53.169631 Epoch 30, Training Loss 0.44179730408865475\n",
      "2022-03-30 18:31:31.414153 Epoch 40, Training Loss 0.3327786143573806\n",
      "2022-03-30 18:34:07.274602 Epoch 50, Training Loss 0.27365949068723433\n",
      "2022-03-30 18:36:43.339798 Epoch 60, Training Loss 0.23048554115054554\n",
      "2022-03-30 18:39:18.582540 Epoch 70, Training Loss 0.17883013456088045\n",
      "2022-03-30 18:41:54.895712 Epoch 80, Training Loss 0.16226256753632423\n",
      "2022-03-30 18:44:30.335085 Epoch 90, Training Loss 0.13333429456354998\n",
      "2022-03-30 18:47:05.777165 Epoch 100, Training Loss 0.10139390078666227\n",
      "2022-03-30 18:49:45.232996 Epoch 110, Training Loss 0.11396011390099231\n",
      "2022-03-30 18:52:25.213393 Epoch 120, Training Loss 0.09061609484824588\n",
      "2022-03-30 18:55:05.072906 Epoch 130, Training Loss 0.09835557351389047\n",
      "2022-03-30 18:57:43.102602 Epoch 140, Training Loss 0.11893834386025663\n",
      "2022-03-30 19:00:19.210224 Epoch 150, Training Loss 0.051839970486864444\n",
      "2022-03-30 19:02:54.591927 Epoch 160, Training Loss 0.051749855483574865\n",
      "2022-03-30 19:05:28.597441 Epoch 170, Training Loss 0.1073719243878675\n",
      "2022-03-30 19:08:02.953192 Epoch 180, Training Loss 0.04732797248519736\n",
      "2022-03-30 19:10:36.733411 Epoch 190, Training Loss 0.027152738879456947\n",
      "2022-03-30 19:13:10.943309 Epoch 200, Training Loss 0.043880363506407304\n",
      "2022-03-30 19:15:47.205331 Epoch 210, Training Loss 0.06977546807459888\n",
      "2022-03-30 19:18:20.724095 Epoch 220, Training Loss 0.046863036547948114\n",
      "2022-03-30 19:20:53.752506 Epoch 230, Training Loss 0.08400599799348789\n",
      "2022-03-30 19:23:28.577772 Epoch 240, Training Loss 0.026719713313084873\n",
      "2022-03-30 19:26:02.762847 Epoch 250, Training Loss 0.006578622488830107\n",
      "2022-03-30 19:28:37.808932 Epoch 260, Training Loss 0.0011851692357185922\n",
      "2022-03-30 19:31:12.229600 Epoch 270, Training Loss 0.0006375230191119872\n",
      "2022-03-30 19:33:47.186230 Epoch 280, Training Loss 0.0005234515330040172\n",
      "2022-03-30 19:36:21.715713 Epoch 290, Training Loss 0.00044796918913343314\n",
      "2022-03-30 19:38:55.369326 Epoch 300, Training Loss 0.0004236004865223691\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec1e89fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.66\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted==labels).sum())\n",
    "                \n",
    "        print(\"Accuracy {}: {:.2f}\".format(name, correct/total))\n",
    "        \n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ed10c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                         for p in model.parameters())\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 ==0:\n",
    "            print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(),\n",
    "                                                        epoch,\n",
    "                                                        loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e443b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 19:42:36.305529 Epoch 1, Training Loss 1.9613920851131839\n",
      "2022-03-30 19:45:07.212596 Epoch 10, Training Loss 0.952520680397063\n",
      "2022-03-30 19:47:54.860173 Epoch 20, Training Loss 0.7531710693141078\n",
      "2022-03-30 19:50:44.931584 Epoch 30, Training Loss 0.6405774277189503\n",
      "2022-03-30 19:53:33.400313 Epoch 40, Training Loss 0.5668739221056404\n",
      "2022-03-30 19:56:21.793212 Epoch 50, Training Loss 0.5318197994814504\n",
      "2022-03-30 19:59:10.588577 Epoch 60, Training Loss 0.506883549835066\n",
      "2022-03-30 20:01:58.880331 Epoch 70, Training Loss 0.48401006480769426\n",
      "2022-03-30 20:04:46.583662 Epoch 80, Training Loss 0.48376262847267454\n",
      "2022-03-30 20:07:35.313551 Epoch 90, Training Loss 0.4636950268388709\n",
      "2022-03-30 20:10:24.326739 Epoch 100, Training Loss 0.4620126979735196\n",
      "2022-03-30 20:13:13.363692 Epoch 110, Training Loss 0.43894982391306203\n",
      "2022-03-30 20:16:02.578973 Epoch 120, Training Loss 0.45495226845869324\n",
      "2022-03-30 20:18:51.940689 Epoch 130, Training Loss 0.4355231932819347\n",
      "2022-03-30 20:21:39.883579 Epoch 140, Training Loss 0.43140800693608305\n",
      "2022-03-30 20:24:28.380067 Epoch 150, Training Loss 0.42585280556660476\n",
      "2022-03-30 20:27:17.023828 Epoch 160, Training Loss 0.432812912956528\n",
      "2022-03-30 20:30:06.474465 Epoch 170, Training Loss 0.4232579141169253\n",
      "2022-03-30 20:32:54.004356 Epoch 180, Training Loss 0.3984303347518682\n",
      "2022-03-30 20:35:45.832177 Epoch 190, Training Loss 0.41785313192840734\n",
      "2022-03-30 20:38:34.899207 Epoch 200, Training Loss 0.3855384679706505\n",
      "2022-03-30 20:41:23.077642 Epoch 210, Training Loss 0.40261622504962374\n",
      "2022-03-30 20:44:14.797952 Epoch 220, Training Loss 0.40925702136343395\n",
      "2022-03-30 20:47:04.167512 Epoch 230, Training Loss 0.392023106593915\n",
      "2022-03-30 20:49:54.957523 Epoch 240, Training Loss 0.42115905381681973\n",
      "2022-03-30 20:52:47.497077 Epoch 250, Training Loss 0.3779370368594099\n",
      "2022-03-30 20:55:37.331431 Epoch 260, Training Loss 0.37091208437976936\n",
      "2022-03-30 20:58:26.460158 Epoch 270, Training Loss 0.37940993642105775\n",
      "2022-03-30 21:01:17.069748 Epoch 280, Training Loss 0.36809516547585996\n",
      "2022-03-30 21:04:06.854592 Epoch 290, Training Loss 0.38987099141111153\n",
      "2022-03-30 21:06:58.372147 Epoch 300, Training Loss 0.3286065761847874\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e04b19e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.92\n",
      "Accuracy val: 0.66\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ed0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3,n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * n_chans // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetResDeep()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6077d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                          shuffle=False)\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3,n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * n_chans // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbeb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetResDeep()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
